# deode_harp_plugin
Plugin interface to run harp point verification for deode extreme weather cases.

# Prerrequisites:
- An installation of harpv02.2 or higher. Instructions can be found here: https://harphub.github.io/harp_training_2024/get-started.html
- Get the official harp scripts for operational use, available in https://github.com/harphub/oper-harp-verif
- There are some extra R packages needed by oper-harp-verif which are not usually installed with harp: "here", "argparse", "pracma" , "pals", "shinyWidgets", "cowplot", "scico", "sf". Install in R console with install.packages("here") etc...
- Load the R module by default in your $HOME/.bahsrc
  
# How to use:
This plugin needs the config.toml file from the deode run that you want to verify as input. Relevant configuration of the verification paths, etc. must be updated in the file harpverify_plugin.toml. 

To create a verification suite from a config.toml and the harpverify_plugin.toml file, create a file -e.g. called configuration- in the Deode-Workflow home directory, with the following content:
```
--config-file
  /path/to/config.toml
  /path/to/harpverify_plugin.toml
```
Check for the comments in harpverify_plugin.toml to understand how to set each plugin/user specific variable:
```
[general.plugin_registry.plugins]
  harpverify = "/home/sp3c/deode_project/deode_plugins/"
(...)
[scheduler.ecfvars]
  case_prefix = "HARPVERIF_" # Add this to the suite name to not collide with OD case runs.
  ecf_host = "ecflow-gen-sp3c-001" # Hardcode your ecflow server (needed after v0.12.0)
(...)
[submission.harpverify_group.ENV]
  DEODE = "awesome"
  DUSER = "rm6"   # DEODE User: who ran the DEODE case (you or other user)
  HUSER = "sp3c"   # HARP User : who runs the HARP verification (your user)
  HARPSCRIPTS_HOME = "/home/sp3c/deode_project/oper-harp-verif/" # Path to your HARP SCRIPTS installation
  VERIF_HOME= "/ec/res4/hpcperm/sp3c/deode_project/deode_harp_output/" # Root path for HARP results, config files, OBSTABLES and FCTABLES...
  REF_SQLITES="/ec/res4/hpcperm/aut6432/DE_Verification/FCTABLES/" # Where to find sqlites for the reference models
  #OBSTABLES_PATH="/ec/res4/hpcperm/sp3c/deode_project/deode_harp_output/OBSTABLE_MERGED/"
  OBSTABLES_PATH="/ec/res4/hpcperm/aut6432/DE_Verification/OBSTABLES/" # Do not change, these are the official OBSTABLES files for verifications.
  REF_NAME=["GDT_iekm","IFS"] # Names of your reference models
  ECFS_ARCHIVE_RELPATH_HARPOUTPUT="/deode/HARP_VERIF/cases/" # Root to store the HARP results in your permanent archive
  ECFS_ARCHIVE_RELPATH_DEODEOUTPUT="/deode/" #Normally /deode/ for verifyng experiments from generic users, /DE_NWP/deode/ to locate FCTABLES and config files from operational user (aut6432)
  DCMDB_DIR="/home/sp3c/deode_project/dcmdb/" #DCMDB installation. Only used if running the batch script for On-duty runs at once
  DW_DIR="/home/sp3c/deode_project/Deode-Workflow" # Deode-Workflow installation
  RDSS_PATH="/ec/res4/hpcperm/sp3c/deode_project/deode_harp_output/cases/iekm/" # Path to store your HARP results (in subfolders for each case)
  PNGS_PATH="/ec/res4/hpcperm/sp3c/deode_project/deode_harp_output/casesplots/iekm/" # Path to store your HARP figures (in subfolders for each case)
  USE_OPERATIONAL_INDEXING="no" # Whether to use the Operational indexing (use in combination with operational DUSER=aut6432)
  OBS_STEP=3 # Frequecy of observations. Use 1 or 3 hours, tipically
(...)
```
Make sure to create the paths VERIF_HOME, 

The plugin is normally used for verification of the DEODE runs by the on-duty team (DUSER=aut6432), which use a specific way of indexing/archiving runs

If you want to use this plugin to run verifications for one of your own cases, set DUSER=HUSER=your_user and USE_OPERATIONAL_INDEXING=no
## References
It is possible to use more than one reference in the verifications. The reference sqlites should all be available in the path set in REF_SQLITES, and the names of the reference models is set in a list in REF_NAME (both variables set in harpverify_plugin.toml Currently GlobalDT_iekm and IFS are used, for which a daily extraction & interpolation for verification has been set up on the on-duty team user using [extract_dt_plugin](https://github.com/destination-earth-digital-twins/extract_dt_plugin)

Currently the harpverify_plugin.toml is set to use the Global Digital Twin (GDT_iekm) and IFS as references, and the path for the sqlites of these references points to where they are routinely generated by DEODE's operational user (aut6432). They are available for dates after Feb,1, 2025. 
## Scaling units
Finally, for solving some discrepancies with model units of a few variables between these references and the OD runs, a careful setting of scaling of units must be set in the file verification/set_params.R from oper-harp-verif. A copy of the currently operational one can be found at /home/aut6432/DE_Verification/verif_tools/oper-harp-verif/verification/set_params.R
## How to launch your verification suite
NOTE: The verification plugin looks for the information stored in the TOML config file of the case to verify and creates a verification suite plugin using the Deode-Workflow. It is highly recommended that the Deode-Workflow version (tag or commmit) installed in your system is the same as the one used to run the DEODE case. 

After setting your harpverify_plugin.toml correctly, create and launch the verification suite with these commands: 
```
 poetry shell (or, for more recent DW installs, `poetry env activate` + source the output of the previous command
 
 deode case ?configuration -o verification_suite.toml
 
 deode start suite --config-file verification_suite.tom

```
## Automatic scripts

### For single verifications 
The script launch_from_user.py can be used to run a single verification of a DEODE experiment run by yourself or some other user. It automatically writes the configuration file and launches the verification suite. This can be use to verify your runs or some colleague's runs. 
For this purpose, make sure to set ECFS_ARCHIVE_RELPATH_DEODEOUTPUT="/deode/" and USE_OPERATIONAL_INDEXING="no" in harpverify_plugin.toml. Make sure to have your poetry environment activated for DW.

This script can also be used to run a single verification ran by the on-duty team (typically using the operational account, currently aut6432 on ATOS), setting "/DE_NWP/deode/" and "yes" in the previous two variables.

The script is run like this:
```
 python3 launch_from_user.py -h
usage: python3 launch_from_user.py [-h] [--event_type EVENT_TYPE] [--order ORDER] [--csc_res CSC_RES] config_file deode_user experiment year month day

Prepare and submit harpverify experiments using DEODE.

positional arguments:
  config_file           Path to harpverify_plugin.toml
  deode_user            DEODE user
  experiment            Experiment name
  year                  Year (YYYY)
  month                 Month (MM)
  day                   Day (DD)

options:
  -h, --help            show this help message and exit
  --event_type EVENT_TYPE
                        Event type (required if indexing is 'yes')
  --order ORDER         Order (required if indexing is 'yes')
  --csc_res CSC_RES     CSC resolution (required if indexing is 'yes')
```
Where experiment_name is generally the name of the suite of the case run with DW, which is also the name used to store the run in ec:../{deode_user}/deode/

The three optional arguments are to be used when verifying aut6432's indexed runs: refer to dcmdb or to the weekly logs by the on-duty team to get info about these runs.

If everything went fine, a new suite will appear in your ecflow_ui, named just like the deode case, with a family called "Case_point_verification" and tasks to get the verification files, Verify, and save the files conveniently:

![Screenshot from 2024-10-21 10-59-18](https://github.com/user-attachments/assets/f68f5f10-2488-437b-932d-709bd8914d60)

### For running verifications indexed in dcmdb
In addition, for automatizing verifications run by the on-duty team, an auxiliar script is available (launch_from_dcmdb.py) to read new runs betweeen two dates from dcmdb,
download the config files and launch all the suites. Make sure that [dcmdb](https://github.com/destination-earth-digital-twins/dcmdb) is installed in your system. The script is run like this:
```
launch_from_dcmdb.py path/to/harpverify_plugin.toml start_date end_date
```
Finally , it must be noted that, since these verifications are configured from the original toml files that ran the case, and the plugin makes use of the DW scripting system, it is highly recommended to use the same DW tag that originally ran the OD case.
Otherwise, very often something will go wrong when creating the suites, given the very dynamic evolution of the DW. To know which tag was used for a specific OD case, look for "describe" in its config.toml file


